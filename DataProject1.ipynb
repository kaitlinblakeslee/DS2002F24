{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPc79Nv0ncuJkSy2cCi6lKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaitlinblakeslee/DS2002F24/blob/main/DataProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exa6Q1AhF-n_",
        "outputId": "28ec979b-2bfd-4159-dc39-4ba61472f7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose your inputadvice_local_file.json\n",
            "Choose your input typejson\n",
            "Choose a target ouputadvice.sql\n",
            "Choose your output typesqlite\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data processor should be able to fetch/download/retrieve local file or remote URL\n",
        "\n",
        "import csv\n",
        "import sqlite3\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Ask the user for an input and an output data file and data type\n",
        "\n",
        "input_file = input('Choose your input')\n",
        "input_type = input('Choose your input type')\n",
        "output_file = input('Choose a target ouput')\n",
        "output_type = input('Choose your output type')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your API URL - this is specifically for my JSON file which I got from the internet (a url)\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://api.adviceslip.com/advice'\n",
        "\n",
        "# Fetch the data\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Write the data to a local file\n",
        "with open(\"advice_local_file.json\", \"w\") as advice_json_file:\n",
        "    json.dump(data, advice_json_file, indent=4)"
      ],
      "metadata": {
        "id": "yyfrEk3WiBrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert general format and data structure of the data source\n",
        "\n",
        "# conversion function should do every single type of conversion I need\n",
        "\n",
        "def convert_data(input_file, input_type, output_file, output_type):\n",
        "  if input_type == 'csv' and output_type == 'json': #csv to json example\n",
        "    with open(input_file, 'r') as csv_file:\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "        data = [row for row in csv_reader]\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(data, json_file)\n",
        "    print(\"CSV to JSON conversion successful!\")\n",
        "  elif input_type == 'json' and output_type == 'csv': #json to csv example\n",
        "    with open(input_file, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    with open(output_file, 'w', newline='') as csv_file:\n",
        "        csv_writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
        "        csv_writer.writeheader()\n",
        "        csv_writer.writerows(data)\n",
        "    print(\"JSON to CSV conversion successful!\")\n",
        "  elif input_type == 'json' and output_type == 'sqlite': #json to sqlite example\n",
        "    with open(input_file, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    with sqlite3.connect(output_file) as conn:\n",
        "        df = pd.DataFrame(data)\n",
        "        conn = sqlite3.connect(output_file)\n",
        "        # need to define the table\n",
        "        table = 'advice'\n",
        "        df.to_sql(table, conn, if_exists='replace', index=False)\n",
        "        conn.close()\n",
        "    print(\"JSON to SQLite conversion successful!\")\n",
        "  elif input_type == 'sqlite' and output_type == 'csv': #sqlite to csv example\n",
        "    with sqlite3.connect(input_file) as conn:\n",
        "        table = 'advice' # define table again\n",
        "        df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(\"SQLite to CSV conversion successful!\")\n",
        "  else:\n",
        "    print(\"Invalid conversion\") #handle errors - this is a way to deal with errors\n",
        "\n",
        "convert_data(input_file, input_type, output_file, output_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wkZ7Jg-cVQO",
        "outputId": "4c5ccfee-020d-4fce-a50d-5c19f6493834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON to SQLite conversion successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the number of columns from the source to the destination\n",
        "\n",
        "#Modify csv (source) dataset (WHR2023 - World Happiness Ratings) OR JSON data (depending on user's original input)\n",
        "\n",
        "# csv deletion of columns\n",
        "columns_to_delete = ['upperwhisker', 'lowerwhisker'] #specifically delete the upperwhisker and lowerwhisker columns from the happiness csv data\n",
        "\n",
        "# json modification - did not add a column because nested within a dictionary (actually added a record and deleted a column)\n",
        "new_columns = {\n",
        "    \"happy thoughts\": \"Golden Retriever puppies\",\n",
        "    \"Rating\": \"Please leave a review and rate how helpful our advice was\"\n",
        "}\n",
        "\n",
        "# Modify the number of columns function - handles inputs that are csvs or jsons\n",
        "\n",
        "def modify_columns(input_file, input_type, output_file, output_type):\n",
        "  if input_type == 'csv' and output_type == 'json':\n",
        "    df = pd.read_csv(input_file)\n",
        "    if columns_to_delete:\n",
        "      df = df.drop(columns=columns_to_delete)\n",
        "    df.to_json(output_file, orient='records')\n",
        "    print(\"CSV to JSON conversion with column deletion successful!\")\n",
        "  elif input_type == 'json' and output_type == 'sqlite':\n",
        "    with open(input_file, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "\n",
        "# Check if 'slip' key exists and if it's a dictionary\n",
        "\n",
        "    if 'slip' in data and isinstance(data['slip'], dict):\n",
        "        # Apply column additions to the nested dictionary under 'slip'\n",
        "        for key, value in new_columns.items():\n",
        "            data['slip'][key] = value  # Modify this line to reflect correct path\n",
        "        print(\"JSON to SQLite conversion with column addition successful!\")\n",
        "  else:\n",
        "    print(\"Invalid conversion\") # deal with errors\n",
        "\n",
        "modify_columns(input_file, input_type, output_file, output_type)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQdZcOozgWpX",
        "outputId": "34a86d0f-e0d8-40f9-857e-bd449df995db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON to SQLite conversion with column addition successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store : the converted file should be written to a SQL database\n",
        "\n",
        "#the json file that was converted to a sql is already written to a SQL database (advice.sql)\n",
        "\n",
        "# however the csv file that was converted to a json has not yet been written to a SQL database\n",
        "\n",
        "# write JSON file (WHR2023.json) to SQL database\n",
        "\n",
        "# first load converted JSON file\n",
        "with open(\"WHR2023.json\", \"r\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# connect to SQLite\n",
        "conn = sqlite3.connect('WHR.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# create table dynamically based on the JSON\n",
        "columns = data[0].keys()  # Extract column names from the JSON keys\n",
        "column_definitions = \", \".join([f\"'{col}' TEXT\" for col in columns])  # experienced issue with syntax so had to add 'back ticks' around column\n",
        "\n",
        "# create table\n",
        "cursor.execute(f\"CREATE TABLE IF NOT EXISTS WHR_table ({column_definitions})\")\n",
        "\n",
        "# insert data from JSON into table\n",
        "for item in data:\n",
        "    values = [str(item[col]) for col in columns]\n",
        "    placeholders = \", \".join([\"?\"] * len(columns))\n",
        "    cursor.execute(f\"INSERT INTO WHR_table ({', '.join([f'[{col}]' for col in columns])}) VALUES ({placeholders})\", values) #also needed to enclose column names here\n",
        "\n",
        "# commit changes and close connection\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"Data from JSON file has been successfully written to a SQL database\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j3A6x-9dyF2",
        "outputId": "6f8720ee-b088-4704-bb96-f0b2ddf276e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data from JSON file has been successfully written to a SQL database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data File ingestion summary : number of records and number of columns\n",
        "\n",
        "# First - data file ingestion summary for csv\n",
        "with open(\"WHR2023.csv\", newline='') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    csv_data = list(reader)  # Convert to a list of rows\n",
        "\n",
        "num_csv_records = len(csv_data)  # Number of records\n",
        "num_csv_columns = len(csv_data[0])  # Number of columns\n",
        "\n",
        "print(f\"CSV Data Ingestion Summary: {num_csv_records} records, {num_csv_columns} columns.\")\n",
        "\n",
        "# Second - data file ingestion summary for json\n",
        "\n",
        "with open(\"advice_local_file.json\", \"r\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "num_records = 1  # Number of records - this is because all of the data is nested in a dictionary ('slip')\n",
        "num_columns = len(data['slip'].keys())  # Number of columns\n",
        "\n",
        "print(f\"JSON Data Ingestion Summary: {num_records} records, {num_columns} columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjNZJ1aOnEcy",
        "outputId": "0905e14d-9485-4ab5-8f26-3214314e90bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Data Ingestion Summary: 138 records, 19 columns.\n",
            "JSON Data Ingestion Summary: 1 records, 2 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data File processing summary : number of records and number of columns\n",
        "\n",
        "# First - data file processing summary for csv to json to sql\n",
        "\n",
        "conn = sqlite3.connect('WHR.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Count the number of records in the SQL table\n",
        "cursor.execute('SELECT COUNT(*) FROM WHR_table')\n",
        "num_records_sql = cursor.fetchone()[0]\n",
        "\n",
        "# Count the number of columns in the SQL table\n",
        "cursor.execute('PRAGMA table_info(WHR_table)')\n",
        "num_columns_sql = len(cursor.fetchall())\n",
        "\n",
        "print(f\"Post-Processing Summary: {num_records_sql} records, {num_columns_sql} columns.\")\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozo2jtMMnJrr",
        "outputId": "3a271869-8ae1-4659-bc6a-2b7ff292456c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-Processing Summary: 274 records, 17 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second - data file processing summary for json to sql\n",
        "\n",
        "conn = sqlite3.connect('advice.sql')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Count the number of records in the SQL table\n",
        "cursor.execute('SELECT COUNT(*) FROM advice')\n",
        "num_records_sql = cursor.fetchone()[0]\n",
        "\n",
        "# Count the number of columns in the SQL table\n",
        "cursor.execute('PRAGMA table_info(advice)')\n",
        "num_columns_sql = len(cursor.fetchall())\n",
        "\n",
        "print(f\"Post-Processing Summary: {num_records_sql} records, {num_columns_sql} columns.\")\n",
        "\n",
        "# Close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZiOEUXdpJpF",
        "outputId": "912c7e47-b961-4e47-ce9a-945cce6fecb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-Processing Summary: 2 records, 1 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Page Reflection:\n",
        "\n",
        "For me personally with minimal coding experience, this task was extremely daunting at first. I think I originally was confused at the task at hand and the logic for what exactly I had to accomplish. However, once I began creating the ETL processor, I started to get a better sense of the assignment, and it started to get easier. As a whole, I feel that I learned a lot through this project about different types of data and how to handle/convert them, which is a useful skill for any data science project.\n",
        "\n",
        "The datasets I used were a ‘World Happiness Ratings’ csv file from Kaggle and an API url from the public github about ‘advice’. I initially thought that reading the data and getting the user inputs would be the easiest parts of the project, however I actually struggled with this quite a bit. I think starting any code is challenging because at this point I was still thinking through the steps I needed to take to create the processor. I named the input for my file ‘inputfile’, and I think this was interfering with the input function, because I originally kept getting an error. I changed the name to ‘input_file’, and this worked better. Additionally, even though we practiced API calls in class, I still don’t feel too comfortable yet working with API and JSON files, so converting the url to a local JSON file was also challenging for me. Going into the project, I thought that converting between file types would be the hardest part. However once I created the function and got started, it actually became quite repetitive. I think that creating a function to do this with ‘if’ statements made the most sense for this assignment. I also used ‘else’ to handle possible errors or if there is an unfamiliar data type. I forgot that I needed a table for the SQL data, so I did have to go back and define a table within the function. The modification for the csv file was fairly simple. For the JSON, on the other hand, I found myself very confused. This was because the JSON was made of a dictionary of dictionaries (and also some strings), so it was difficult to access the data directly. Instead, I used the ‘slip’ key to get inside the dictionary. I think spending more time familiarizing myself with the data in the beginning would’ve been beneficial in this case. Writing the original CSV to a SQL database was also a little challenging in that I thought I was going to have to create the table from scratch and add all the columns and values, which would’ve been extremely time consuming (and I did start to do this). However, then I realized I could just build the table dynamically from the JSON data, which was a lot quicker! Lastly, retrieving the data file ingestion and processing summary at the very end was probably the smoothest part of this project.\n",
        "\n",
        "I think this data processing utility will be extremely useful for future data science projects. Often, the data is not presented in the format that is needed for a specific task or analytic model. So being able to convert relatively easily between data types using this processor is a great tool to have. While the datasets I used only had slight correlations to one another (world happiness ratings and therapeutic advice), I could see this tool being helpful when trying to compare similar datasets of different formats as well.\n"
      ],
      "metadata": {
        "id": "Q-mQ8g492mTj"
      }
    }
  ]
}